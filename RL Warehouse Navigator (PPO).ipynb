{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465b2a15-cc5f-4986-b9aa-b4e9e0f2247d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lightweight PPO...\n",
      "Using cpu device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 40       |\n",
      "|    ep_rew_mean     | -3.98    |\n",
      "| time/              |          |\n",
      "|    fps             | 587      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 128      |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 33.6          |\n",
      "|    ep_rew_mean          | -1.86         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 406           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 0             |\n",
      "|    total_timesteps      | 256           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017357292 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 0.122         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.304         |\n",
      "|    n_updates            | 3             |\n",
      "|    policy_gradient_loss | -0.0016       |\n",
      "|    value_loss           | 0.848         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 35.4          |\n",
      "|    ep_rew_mean          | -1.99         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 374           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 1             |\n",
      "|    total_timesteps      | 384           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.9462618e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -0.0804       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.26          |\n",
      "|    n_updates            | 6             |\n",
      "|    policy_gradient_loss | -0.000265     |\n",
      "|    value_loss           | 3.64          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 36.6         |\n",
      "|    ep_rew_mean          | -2.17        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 368          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 512          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.318923e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | -0.0522      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.72         |\n",
      "|    n_updates            | 9            |\n",
      "|    policy_gradient_loss | -0.000889    |\n",
      "|    value_loss           | 2.15         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 34.8        |\n",
      "|    ep_rew_mean          | -1.75       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 330         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 640         |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 6.74217e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.42        |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | -0.000678   |\n",
      "|    value_loss           | 1.9         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 34.9         |\n",
      "|    ep_rew_mean          | -1.57        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 308          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 768          |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001494214 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | 0.063        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.48         |\n",
      "|    n_updates            | 15           |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    value_loss           | 3.56         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 32.2          |\n",
      "|    ep_rew_mean          | -0.878        |\n",
      "| time/                   |               |\n",
      "|    fps                  | 301           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 896           |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012457184 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 0.0955        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.39          |\n",
      "|    n_updates            | 18            |\n",
      "|    policy_gradient_loss | -0.00108      |\n",
      "|    value_loss           | 3.84          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 32.9          |\n",
      "|    ep_rew_mean          | -1.1          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 297           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 1024          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4627818e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -0.0894       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.94          |\n",
      "|    n_updates            | 21            |\n",
      "|    policy_gradient_loss | 0.000112      |\n",
      "|    value_loss           | 5.64          |\n",
      "-------------------------------------------\n",
      "\n",
      "=== Play one episode ===\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.050\n",
      ". . . . . .\n",
      ". . . . . .\n",
      ". . A . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.150\n",
      ". . . . . .\n",
      ". . A . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.150\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.150\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n",
      ". . A . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "T . . . . .\n",
      ". . . . . .\n",
      ". . . . . .\n",
      "reward=-0.100\n"
     ]
    }
   ],
   "source": [
    "# warehouse_rl_light.py\n",
    "import random\n",
    "from typing import Tuple, Dict, Any, List\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# ----------------- Environment --------------------\n",
    "\n",
    "class WarehouseMultiRobotEnv(gym.Env):\n",
    "    \"\"\"Lightweight single-robot warehouse grid environment for debugging.\"\"\"\n",
    "    metadata = {\"render_modes\": [\"ansi\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, width=6, height=6, num_robots=1, max_steps=40, seed=7):\n",
    "        super().__init__()\n",
    "        assert num_robots == 1, \"This debug version supports 1 robot only\"\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.num_robots = num_robots\n",
    "        self.max_steps = max_steps\n",
    "        self._rng = np.random.default_rng(seed)\n",
    "\n",
    "        # Actions: 0=stay,1=up,2=down,3=left,4=right\n",
    "        self._single_actions = [(0, 0), (0, -1), (0, 1), (-1, 0), (1, 0)]\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "\n",
    "        # Observation: robot_pos (x,y), target_pos (x,y), steps_remaining_norm\n",
    "        obs_dim = 2 + 2 + 1\n",
    "        self.observation_space = spaces.Box(0.0, 1.0, shape=(obs_dim,), dtype=np.float32)\n",
    "\n",
    "        self.robot_pos: List[Tuple[int, int]] = []\n",
    "        self.target_pos: Tuple[int, int] = (0, 0)\n",
    "        self.steps = 0\n",
    "        self._done_robot = False\n",
    "        self._prev_dist = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        if seed is not None:\n",
    "            self._rng = np.random.default_rng(seed)\n",
    "\n",
    "    def _sample_free_cell(self):\n",
    "        return int(self._rng.integers(0, self.width)), int(self._rng.integers(0, self.height))\n",
    "\n",
    "    def _place_entities(self):\n",
    "        self.robot_pos = [self._sample_free_cell()]\n",
    "        while True:\n",
    "            t = self._sample_free_cell()\n",
    "            if t != self.robot_pos[0]:\n",
    "                self.target_pos = t\n",
    "                break\n",
    "\n",
    "    def _normalize(self, x, y):\n",
    "        return x / (self.width - 1), y / (self.height - 1)\n",
    "\n",
    "    def _obs(self):\n",
    "        r1x, r1y = self.robot_pos[0]\n",
    "        tx, ty = self.target_pos\n",
    "        steps_remaining_norm = (self.max_steps - self.steps) / max(1, self.max_steps)\n",
    "        return np.array([*self._normalize(r1x, r1y), *self._normalize(tx, ty), steps_remaining_norm], dtype=np.float32)\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        self._place_entities()\n",
    "        self.steps = 0\n",
    "        self._done_robot = False\n",
    "        self._prev_dist = self._manhattan(self.robot_pos[0], self.target_pos)\n",
    "        return self._obs(), {\"robot_pos\": list(self.robot_pos), \"target_pos\": self.target_pos}\n",
    "\n",
    "    def _manhattan(self, a, b):\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def _apply_move(self, pos, move_idx):\n",
    "        dx, dy = self._single_actions[move_idx]\n",
    "        nx = min(max(pos[0] + dx, 0), self.width - 1)\n",
    "        ny = min(max(pos[1] + dy, 0), self.height - 1)\n",
    "        return nx, ny\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        next_pos = self._apply_move(self.robot_pos[0], action)\n",
    "        self.robot_pos[0] = next_pos\n",
    "\n",
    "        goal_reward = 0.0\n",
    "        if not self._done_robot and self.robot_pos[0] == self.target_pos:\n",
    "            self._done_robot = True\n",
    "            goal_reward = 5.0\n",
    "\n",
    "        step_penalty = -0.1 if not self._done_robot else 0.0\n",
    "        new_dist = self._manhattan(self.robot_pos[0], self.target_pos)\n",
    "        shaping = 0.05 * (self._prev_dist - new_dist)\n",
    "        self._prev_dist = new_dist\n",
    "\n",
    "        reward = step_penalty + shaping + goal_reward\n",
    "        terminated = self._done_robot\n",
    "        truncated = self.steps >= self.max_steps\n",
    "        info = {\"robot_pos\": list(self.robot_pos), \"target_pos\": self.target_pos}\n",
    "\n",
    "        return self._obs(), float(reward), bool(terminated), bool(truncated), info\n",
    "\n",
    "    def render(self):\n",
    "        grid = [[\".\" for _ in range(self.width)] for _ in range(self.height)]\n",
    "        tx, ty = self.target_pos\n",
    "        grid[ty][tx] = \"T\"\n",
    "        x, y = self.robot_pos[0]\n",
    "        grid[y][x] = \"A\" if not self._done_robot else \"a\"\n",
    "        s = \"\\n\".join(\" \".join(row) for row in grid[::-1])\n",
    "        return s\n",
    "\n",
    "# ----------------- Utilities --------------------\n",
    "\n",
    "def make_env_fn():\n",
    "    def _thunk():\n",
    "        env = WarehouseMultiRobotEnv()\n",
    "        return Monitor(env)\n",
    "    return _thunk\n",
    "\n",
    "def make_vec_env(n_envs=1):\n",
    "    thunks = [make_env_fn() for _ in range(n_envs)]\n",
    "    return DummyVecEnv(thunks)\n",
    "\n",
    "def train_ppo(total_timesteps=1000):\n",
    "    env = make_vec_env(n_envs=1)\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1, n_steps=128, n_epochs=3, batch_size=32)\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    return model, env\n",
    "\n",
    "def play_one_episode(model, render=True):\n",
    "    vec_env = model.get_env()\n",
    "    env = vec_env.envs[0].env  # unwrap\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    traj = []\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(int(action))\n",
    "        done = terminated or truncated\n",
    "        traj.append({\"reward\": reward, \"robot_pos\": info.get(\"robot_pos\"), \"target_pos\": info.get(\"target_pos\")})\n",
    "        if render:\n",
    "            print(env.render())\n",
    "            print(f\"reward={reward:+.3f}\")\n",
    "    return traj\n",
    "\n",
    "# -------------------- Script --------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Training lightweight PPO...\")\n",
    "    model, _ = train_ppo(total_timesteps=1000)\n",
    "    print(\"\\n=== Play one episode ===\")\n",
    "    play_one_episode(model, render=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452609c-a10c-47ce-9178-c8bbeca9bc51",
   "metadata": {},
   "source": [
    "### **Breakdown of Code** \n",
    "\n",
    "---\n",
    "\n",
    "### **1️⃣ Environment Setup**\n",
    "\n",
    "* The environment is a **6×6 warehouse grid**.\n",
    "* There is **1 robot** (`A`) and **1 target** (`T`).\n",
    "* The robot starts at a **random cell**, and the target is placed in a **different random cell**.\n",
    "* **Max steps** per episode = 40.\n",
    "\n",
    "**Observation space:**\n",
    "\n",
    "* `[robot_x, robot_y, target_x, target_y, steps_remaining_norm]`\n",
    "* Normalized between 0 and 1.\n",
    "\n",
    "**Action space:**\n",
    "\n",
    "* `0` = stay\n",
    "* `1` = up\n",
    "* `2` = down\n",
    "* `3` = left\n",
    "* `4` = right\n",
    "\n",
    "---\n",
    "\n",
    "### **2️⃣ Reset**\n",
    "\n",
    "* `reset()`:\n",
    "\n",
    "  * Places robot and target randomly.\n",
    "  * Resets steps, reward shaping, and “done” flag.\n",
    "  * Returns the initial observation.\n",
    "\n",
    "---\n",
    "\n",
    "### **3️⃣ Step Function**\n",
    "\n",
    "* `step(action)`:\n",
    "\n",
    "  * Robot moves according to the chosen action.\n",
    "  * Calculates **reward**:\n",
    "\n",
    "    * `+5` if robot reaches target (goal reward).\n",
    "    * `-0.1` per step until target reached (step penalty).\n",
    "    * Small **shaping reward** for getting closer to the target (`0.05 * distance_reduction`).\n",
    "  * Checks if the episode is **terminated** (robot reached target) or **truncated** (max steps reached).\n",
    "  * Returns observation, reward, terminated, truncated, info.\n",
    "\n",
    "---\n",
    "\n",
    "### **4️⃣ Render**\n",
    "\n",
    "* Prints the **grid as ASCII**:\n",
    "\n",
    "  ```\n",
    "  . . . . . .\n",
    "  . . . . . .\n",
    "  . . A . . .\n",
    "  . . . T . .\n",
    "  ```\n",
    "\n",
    "  * `A` = robot\n",
    "  * `T` = target\n",
    "  * `a` = robot after reaching target\n",
    "\n",
    "---\n",
    "\n",
    "### **5️⃣ PPO Training (Optional)**\n",
    "\n",
    "* `train_ppo(total_timesteps=1000)`:\n",
    "\n",
    "  * Uses **Stable-Baselines3 PPO** on **this tiny environment**.\n",
    "  * **Training is extremely light** (short episodes, small batch) so it runs fast.\n",
    "  * Returns a trained model (can predict actions for the robot).\n",
    "\n",
    "---\n",
    "\n",
    "### **6️⃣ Play One Episode**\n",
    "\n",
    "* `play_one_episode(model)`:\n",
    "\n",
    "  * Runs one episode using the trained PPO model (or random moves if untrained).\n",
    "  * Prints the grid and rewards **step by step**.\n",
    "  * Returns the **trajectory**: positions and rewards over time.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Summary**\n",
    "\n",
    "* Your code is **not heavy** anymore: only 1 robot, small grid, small episode length.\n",
    "* It is **fully functional**:\n",
    "\n",
    "  * Robot moves, rewards are computed, episode ends correctly.\n",
    "  * You can **see the robot moving toward the target** via ASCII output.\n",
    "  * It supports **training with PPO** if you want, but for testing, you can skip training and just do random actions.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1dc47c-18ac-4d9b-80ee-59889e9d6e41",
   "metadata": {},
   "source": [
    "## **Line by line Explanation** \n",
    "\n",
    "---\n",
    "\n",
    "## **Imports**\n",
    "\n",
    "```python\n",
    "import random\n",
    "from typing import Tuple, Dict, Any, List\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "```\n",
    "\n",
    "* `random`: Python’s built-in random generator.\n",
    "* `Tuple, Dict, Any, List`: Type hints for better readability.\n",
    "* `gymnasium` and `spaces`: Used to define the **RL environment** and action/observation spaces.\n",
    "* `numpy`: For numerical operations and random number generation.\n",
    "* `stable_baselines3`: PPO algorithm and environment wrappers.\n",
    "* `DummyVecEnv` and `Monitor`: Used for vectorized environments and monitoring rewards/steps.\n",
    "\n",
    "---\n",
    "\n",
    "## **Environment Class**\n",
    "\n",
    "```python\n",
    "class WarehouseMultiRobotEnv(gym.Env):\n",
    "    \"\"\"Lightweight single-robot warehouse grid environment for debugging.\"\"\"\n",
    "    metadata = {\"render_modes\": [\"ansi\"], \"render_fps\": 4}\n",
    "```\n",
    "\n",
    "* Defines your **custom Gym environment**.\n",
    "* Only supports **1 robot**.\n",
    "* `metadata` is used by Gym for rendering info.\n",
    "\n",
    "---\n",
    "\n",
    "### **Initialization**\n",
    "\n",
    "```python\n",
    "def __init__(self, width=6, height=6, num_robots=1, max_steps=40, seed=7):\n",
    "    super().__init__()\n",
    "    assert num_robots == 1, \"This debug version supports 1 robot only\"\n",
    "    self.width = width\n",
    "    self.height = height\n",
    "    self.num_robots = num_robots\n",
    "    self.max_steps = max_steps\n",
    "    self._rng = np.random.default_rng(seed)\n",
    "```\n",
    "\n",
    "* Sets grid size (`width` × `height`) and max episode steps.\n",
    "* Ensures **1 robot only** (simplified version).\n",
    "* Initializes **NumPy random generator** for reproducible randomness.\n",
    "\n",
    "```python\n",
    "    # Actions: 0=stay,1=up,2=down,3=left,4=right\n",
    "    self._single_actions = [(0, 0), (0, -1), (0, 1), (-1, 0), (1, 0)]\n",
    "    self.action_space = spaces.Discrete(5)\n",
    "```\n",
    "\n",
    "* Defines **robot movements**.\n",
    "* `action_space` = 5 discrete moves.\n",
    "\n",
    "```python\n",
    "    # Observation: robot_pos (x,y), target_pos (x,y), steps_remaining_norm\n",
    "    obs_dim = 2 + 2 + 1\n",
    "    self.observation_space = spaces.Box(0.0, 1.0, shape=(obs_dim,), dtype=np.float32)\n",
    "```\n",
    "\n",
    "* **Observation** contains:\n",
    "\n",
    "  1. Robot position `(x, y)`\n",
    "  2. Target position `(x, y)`\n",
    "  3. Steps remaining normalized `[0,1]`\n",
    "* `Box(0,1)` ensures values are floats between 0 and 1.\n",
    "\n",
    "```python\n",
    "    self.robot_pos: List[Tuple[int, int]] = []\n",
    "    self.target_pos: Tuple[int, int] = (0, 0)\n",
    "    self.steps = 0\n",
    "    self._done_robot = False\n",
    "    self._prev_dist = None\n",
    "```\n",
    "\n",
    "* Initializes internal variables:\n",
    "\n",
    "  * `robot_pos`: current robot position\n",
    "  * `target_pos`: goal cell\n",
    "  * `steps`: step counter\n",
    "  * `_done_robot`: True if robot reached target\n",
    "  * `_prev_dist`: stores previous Manhattan distance for reward shaping\n",
    "\n",
    "---\n",
    "\n",
    "### **Seeding**\n",
    "\n",
    "```python\n",
    "def seed(self, seed=None):\n",
    "    if seed is not None:\n",
    "        self._rng = np.random.default_rng(seed)\n",
    "```\n",
    "\n",
    "* Allows resetting random generator for **reproducibility**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Sample Random Cell**\n",
    "\n",
    "```python\n",
    "def _sample_free_cell(self):\n",
    "    return int(self._rng.integers(0, self.width)), int(self._rng.integers(0, self.height))\n",
    "```\n",
    "\n",
    "* Returns a **random (x,y) cell** in the grid.\n",
    "\n",
    "---\n",
    "\n",
    "### **Place Robot and Target**\n",
    "\n",
    "```python\n",
    "def _place_entities(self):\n",
    "    self.robot_pos = [self._sample_free_cell()]\n",
    "    while True:\n",
    "        t = self._sample_free_cell()\n",
    "        if t != self.robot_pos[0]:\n",
    "            self.target_pos = t\n",
    "            break\n",
    "```\n",
    "\n",
    "* Places **robot** at a random cell.\n",
    "* Places **target** at a different random cell.\n",
    "\n",
    "---\n",
    "\n",
    "### **Normalize Function**\n",
    "\n",
    "```python\n",
    "def _normalize(self, x, y):\n",
    "    return x / (self.width - 1), y / (self.height - 1)\n",
    "```\n",
    "\n",
    "* Converts `(x,y)` to **\\[0,1] range** for the RL observation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Observation Function**\n",
    "\n",
    "```python\n",
    "def _obs(self):\n",
    "    r1x, r1y = self.robot_pos[0]\n",
    "    tx, ty = self.target_pos\n",
    "    steps_remaining_norm = (self.max_steps - self.steps) / max(1, self.max_steps)\n",
    "    return np.array([*self._normalize(r1x, r1y), *self._normalize(tx, ty), steps_remaining_norm], dtype=np.float32)\n",
    "```\n",
    "\n",
    "* Returns **current observation**:\n",
    "\n",
    "  * Robot position normalized\n",
    "  * Target position normalized\n",
    "  * Steps remaining normalized\n",
    "\n",
    "---\n",
    "\n",
    "### **Reset Function**\n",
    "\n",
    "```python\n",
    "def reset(self, *, seed=None, options=None):\n",
    "    if seed is not None:\n",
    "        self.seed(seed)\n",
    "    self._place_entities()\n",
    "    self.steps = 0\n",
    "    self._done_robot = False\n",
    "    self._prev_dist = self._manhattan(self.robot_pos[0], self.target_pos)\n",
    "    return self._obs(), {\"robot_pos\": list(self.robot_pos), \"target_pos\": self.target_pos}\n",
    "```\n",
    "\n",
    "* Resets environment at the start of an episode.\n",
    "* Returns **initial observation** and info dict.\n",
    "\n",
    "---\n",
    "\n",
    "### **Manhattan Distance**\n",
    "\n",
    "```python\n",
    "def _manhattan(self, a, b):\n",
    "    return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "```\n",
    "\n",
    "* Returns the **distance between robot and target** for shaping reward.\n",
    "\n",
    "---\n",
    "\n",
    "### **Apply Move**\n",
    "\n",
    "```python\n",
    "def _apply_move(self, pos, move_idx):\n",
    "    dx, dy = self._single_actions[move_idx]\n",
    "    nx = min(max(pos[0] + dx, 0), self.width - 1)\n",
    "    ny = min(max(pos[1] + dy, 0), self.height - 1)\n",
    "    return nx, ny\n",
    "```\n",
    "\n",
    "* Moves the robot by **action delta**.\n",
    "* Prevents robot from going **outside grid**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step Function**\n",
    "\n",
    "```python\n",
    "def step(self, action):\n",
    "    self.steps += 1\n",
    "    next_pos = self._apply_move(self.robot_pos[0], action)\n",
    "    self.robot_pos[0] = next_pos\n",
    "```\n",
    "\n",
    "* **Increment step counter** and update robot position.\n",
    "\n",
    "```python\n",
    "    goal_reward = 0.0\n",
    "    if not self._done_robot and self.robot_pos[0] == self.target_pos:\n",
    "        self._done_robot = True\n",
    "        goal_reward = 5.0\n",
    "```\n",
    "\n",
    "* **Reward for reaching target**: +5 first time only.\n",
    "\n",
    "```python\n",
    "    step_penalty = -0.1 if not self._done_robot else 0.0\n",
    "    new_dist = self._manhattan(self.robot_pos[0], self.target_pos)\n",
    "    shaping = 0.05 * (self._prev_dist - new_dist)\n",
    "    self._prev_dist = new_dist\n",
    "```\n",
    "\n",
    "* **Penalty** for each step: -0.1 until target reached.\n",
    "* **Shaping reward**: small positive reward for getting closer to target.\n",
    "\n",
    "```python\n",
    "    reward = step_penalty + shaping + goal_reward\n",
    "    terminated = self._done_robot\n",
    "    truncated = self.steps >= self.max_steps\n",
    "    info = {\"robot_pos\": list(self.robot_pos), \"target_pos\": self.target_pos}\n",
    "\n",
    "    return self._obs(), float(reward), bool(terminated), bool(truncated), info\n",
    "```\n",
    "\n",
    "* Returns:\n",
    "\n",
    "  1. Observation\n",
    "  2. Reward\n",
    "  3. Terminated flag\n",
    "  4. Truncated flag (max steps reached)\n",
    "  5. Info dict\n",
    "\n",
    "---\n",
    "\n",
    "### **Render**\n",
    "\n",
    "```python\n",
    "def render(self):\n",
    "    grid = [[\".\" for _ in range(self.width)] for _ in range(self.height)]\n",
    "    tx, ty = self.target_pos\n",
    "    grid[ty][tx] = \"T\"\n",
    "    x, y = self.robot_pos[0]\n",
    "    grid[y][x] = \"A\" if not self._done_robot else \"a\"\n",
    "    s = \"\\n\".join(\" \".join(row) for row in grid[::-1])\n",
    "    return s\n",
    "```\n",
    "\n",
    "* Prints **ASCII warehouse grid**.\n",
    "* Robot = `A` (before reaching target), `a` (after).\n",
    "* Target = `T`.\n",
    "\n",
    "---\n",
    "\n",
    "## **Utilities**\n",
    "\n",
    "### **Environment Wrapper**\n",
    "\n",
    "```python\n",
    "def make_env_fn():\n",
    "    def _thunk():\n",
    "        env = WarehouseMultiRobotEnv()\n",
    "        return Monitor(env)\n",
    "    return _thunk\n",
    "```\n",
    "\n",
    "* Creates a **single environment** wrapped with `Monitor`.\n",
    "\n",
    "```python\n",
    "def make_vec_env(n_envs=1):\n",
    "    thunks = [make_env_fn() for _ in range(n_envs)]\n",
    "    return DummyVecEnv(thunks)\n",
    "```\n",
    "\n",
    "* Wraps environment for **vectorized RL training** (even though here only 1 env).\n",
    "\n",
    "---\n",
    "\n",
    "### **Train PPO**\n",
    "\n",
    "```python\n",
    "def train_ppo(total_timesteps=1000):\n",
    "    env = make_vec_env(n_envs=1)\n",
    "    model = PPO(\"MlpPolicy\", env, verbose=1, n_steps=128, n_epochs=3, batch_size=32)\n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    return\n",
    "```\n",
    "\n",
    "\n",
    "model, env\n",
    "\n",
    "````\n",
    "- Trains a **PPO agent** using your environment.\n",
    "- `total_timesteps=1000` is very light, so it trains quickly.\n",
    "- Returns trained `model` and `env`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Play One Episode**\n",
    "```python\n",
    "def play_one_episode(model, render=True):\n",
    "    vec_env = model.get_env()\n",
    "    env = vec_env.envs[0].env  # unwrap\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    traj = []\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(int(action))\n",
    "        done = terminated or truncated\n",
    "        traj.append({\"reward\": reward, \"robot_pos\": info.get(\"robot_pos\"), \"target_pos\": info.get(\"target_pos\")})\n",
    "        if render:\n",
    "            print(env.render())\n",
    "            print(f\"reward={reward:+.3f}\")\n",
    "    return traj\n",
    "````\n",
    "\n",
    "* Runs **one full episode** using the trained model.\n",
    "* Prints **grid and reward** at each step.\n",
    "* Returns **trajectory data** for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## **Main Script**\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Training lightweight PPO...\")\n",
    "    model, _ = train_ppo(total_timesteps=1000)\n",
    "    print(\"\\n=== Play one episode ===\")\n",
    "    play_one_episode(model, render=True)\n",
    "```\n",
    "\n",
    "* Trains the agent.\n",
    "* Plays one episode and prints step-by-step output.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Summary**\n",
    "\n",
    "* This code defines a **mini warehouse environment** with **1 robot**, **random target**, and **reward shaping**.\n",
    "* Supports **training a PPO agent** or running **random/manual episodes**.\n",
    "* Lightweight: **small grid, few steps, 1 robot**.\n",
    "* Output: ASCII grid + reward per step.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6af5b5-d692-48d7-8228-d83d1421ac80",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **100 Interview Questions & Answers – Warehouse RL Project**\n",
    "\n",
    "---\n",
    "\n",
    "## **EASY (1–30)**\n",
    "\n",
    "1. **Q:** What library is used to create the RL environment?\n",
    "   **A:** `gymnasium` (Gym) is used to create the warehouse RL environment.\n",
    "\n",
    "2. **Q:** How many robots does this debug environment support?\n",
    "   **A:** Only **1 robot**.\n",
    "\n",
    "3. **Q:** What is the purpose of `DummyVecEnv`?\n",
    "   **A:** Wraps the environment for vectorized training; allows stable-baselines3 to work with multiple environments.\n",
    "\n",
    "4. **Q:** What RL algorithm is used?\n",
    "   **A:** **Proximal Policy Optimization (PPO)** from `stable_baselines3`.\n",
    "\n",
    "5. **Q:** What is the size of the grid in this environment?\n",
    "   **A:** Default `6x6`.\n",
    "\n",
    "6. **Q:** How are robot actions represented?\n",
    "   **A:** Discrete moves: 0=stay, 1=up, 2=down, 3=left, 4=right.\n",
    "\n",
    "7. **Q:** What does `spaces.Discrete(5)` mean?\n",
    "   **A:** There are 5 discrete possible actions.\n",
    "\n",
    "8. **Q:** How is the observation space defined?\n",
    "   **A:** `[robot_x, robot_y, target_x, target_y, steps_remaining_norm]` as normalized floats.\n",
    "\n",
    "9. **Q:** How are coordinates normalized?\n",
    "   **A:** Divided by `(width-1)` for x and `(height-1)` for y.\n",
    "\n",
    "10. **Q:** What is `Monitor(env)` used for?\n",
    "    **A:** To log rewards, steps, and other info for training and debugging.\n",
    "\n",
    "11. **Q:** How is a random free cell selected?\n",
    "    **A:** Using `np.random.default_rng(seed).integers(0, width/height)`.\n",
    "\n",
    "12. **Q:** What happens in `_place_entities()`?\n",
    "    **A:** Randomly places robot and target ensuring they do not overlap.\n",
    "\n",
    "13. **Q:** What is `_manhattan()` used for?\n",
    "    **A:** Calculates Manhattan distance between robot and target for shaping reward.\n",
    "\n",
    "14. **Q:** What reward does the agent get for reaching the target?\n",
    "    **A:** `+5.0` on first arrival.\n",
    "\n",
    "15. **Q:** What is the step penalty?\n",
    "    **A:** `-0.1` per step until robot reaches target.\n",
    "\n",
    "16. **Q:** What is reward shaping?\n",
    "    **A:** A small positive reward for **reducing distance** to target: `0.05*(prev_dist-new_dist)`.\n",
    "\n",
    "17. **Q:** How is the environment reset?\n",
    "    **A:** `reset()` sets robot and target positions, steps=0, `_done_robot=False`.\n",
    "\n",
    "18. **Q:** What is the max number of steps per episode?\n",
    "    **A:** `max_steps=40`.\n",
    "\n",
    "19. **Q:** What does `terminated` mean in step() output?\n",
    "    **A:** True if the robot reached the target.\n",
    "\n",
    "20. **Q:** What does `truncated` mean?\n",
    "    **A:** True if the max number of steps is reached.\n",
    "\n",
    "21. **Q:** How is the robot prevented from moving outside the grid?\n",
    "    **A:** Using `min(max(...))` in `_apply_move()`.\n",
    "\n",
    "22. **Q:** How is the trajectory recorded?\n",
    "    **A:** Stored in a list of dictionaries with `reward`, `robot_pos`, `target_pos`.\n",
    "\n",
    "23. **Q:** How is the environment rendered?\n",
    "    **A:** ASCII grid with `A` for robot, `T` for target.\n",
    "\n",
    "24. **Q:** What does `MlpPolicy` mean?\n",
    "    **A:** PPO uses a **multi-layer perceptron** neural network for policy.\n",
    "\n",
    "25. **Q:** What is `total_timesteps`?\n",
    "    **A:** Number of environment steps used to train the PPO model.\n",
    "\n",
    "26. **Q:** What is `n_steps` in PPO?\n",
    "    **A:** Number of steps before updating the model.\n",
    "\n",
    "27. **Q:** What does `deterministic=True` mean in `predict()`?\n",
    "    **A:** Always selects the **highest probability action** instead of sampling.\n",
    "\n",
    "28. **Q:** What does `envs[0].env` do?\n",
    "    **A:** Unwraps the vectorized environment to access the original environment.\n",
    "\n",
    "29. **Q:** What is the purpose of `seed()`?\n",
    "    **A:** Ensures **reproducibility** of random numbers.\n",
    "\n",
    "30. **Q:** How is steps remaining normalized?\n",
    "    **A:** `(max_steps - steps) / max(1, max_steps)` gives a value in \\[0,1].\n",
    "\n",
    "---\n",
    "\n",
    "## **MODERATE (31–70)**\n",
    "\n",
    "31. **Q:** Why is reward shaping used?\n",
    "    **A:** To **encourage learning faster** by giving small rewards before reaching the goal.\n",
    "\n",
    "32. **Q:** Explain `_apply_move()` function.\n",
    "    **A:** Adds movement deltas, clamps robot position within grid, returns new coordinates.\n",
    "\n",
    "33. **Q:** What is the purpose of `play_one_episode()`?\n",
    "    **A:** To run the trained agent and visualize its trajectory.\n",
    "\n",
    "34. **Q:** How does PPO update the policy?\n",
    "    **A:** Uses **advantage estimation** and **clipped objective** for stable updates.\n",
    "\n",
    "35. **Q:** How is the initial previous distance `_prev_dist` set?\n",
    "    **A:** On reset, `_manhattan(robot, target)`.\n",
    "\n",
    "36. **Q:** Why is the environment called “lightweight”?\n",
    "    **A:** Because it uses **1 robot**, small grid, few steps for debugging.\n",
    "\n",
    "37. **Q:** Explain `DummyVecEnv` in simple words.\n",
    "    **A:** Allows RL algorithms to treat single and multiple environments in a **uniform way**.\n",
    "\n",
    "38. **Q:** What is `Monitor` used for internally?\n",
    "    **A:** Tracks **episode rewards, lengths, and stats** for logging.\n",
    "\n",
    "39. **Q:** What happens if robot moves into the target after reaching it once?\n",
    "    **A:** No additional reward (`_done_robot=True` ensures +5 only once).\n",
    "\n",
    "40. **Q:** Why is `np.float32` used in observation space?\n",
    "    **A:** Required for **stable-baselines3**, ensures consistent tensor type.\n",
    "\n",
    "41. **Q:** Explain the observation vector structure.\n",
    "    **A:** `[robot_x_norm, robot_y_norm, target_x_norm, target_y_norm, steps_remaining_norm]`.\n",
    "\n",
    "42. **Q:** Why normalize positions?\n",
    "    **A:** Makes learning **scale-independent** and easier for the neural network.\n",
    "\n",
    "43. **Q:** How is reward calculated per step?\n",
    "    **A:** `reward = step_penalty + shaping + goal_reward`.\n",
    "\n",
    "44. **Q:** What is the difference between terminated and truncated?\n",
    "    **A:** `terminated`: task completed.\n",
    "    `truncated`: max steps reached.\n",
    "\n",
    "45. **Q:** What are `_single_actions`?\n",
    "    **A:** Tuples representing `(dx, dy)` for each action.\n",
    "\n",
    "46. **Q:** What is PPO’s `batch_size`?\n",
    "    **A:** Number of experiences per update batch (here `32`).\n",
    "\n",
    "47. **Q:** Why set `n_envs=1`?\n",
    "    **A:** Single environment for **debug/training simplicity**.\n",
    "\n",
    "48. **Q:** What is the main advantage of PPO over other RL algorithms?\n",
    "    **A:** **Stable policy updates**, avoids large destructive gradient steps.\n",
    "\n",
    "49. **Q:** How do you train for more steps?\n",
    "    **A:** Increase `total_timesteps` in `train_ppo()`.\n",
    "\n",
    "50. **Q:** How is trajectory info stored?\n",
    "    **A:** List of dictionaries containing `reward`, `robot_pos`, `target_pos`.\n",
    "\n",
    "51. **Q:** What happens if robot hits wall?\n",
    "    **A:** Robot stays within bounds due to clamping; no penalty.\n",
    "\n",
    "52. **Q:** How to modify grid size?\n",
    "    **A:** Change `width` and `height` in environment init.\n",
    "\n",
    "53. **Q:** How to change max steps?\n",
    "    **A:** Pass `max_steps` argument to environment.\n",
    "\n",
    "54. **Q:** Can PPO handle multiple robots in current code?\n",
    "    **A:** No, this debug version supports **1 robot only**.\n",
    "\n",
    "55. **Q:** What is deterministic vs stochastic policy?\n",
    "    **A:** Deterministic: pick best action.\n",
    "    Stochastic: sample from action probabilities.\n",
    "\n",
    "56. **Q:** What is the learning objective of PPO?\n",
    "    **A:** Maximize expected reward while keeping **policy change small**.\n",
    "\n",
    "57. **Q:** Why is reward negative for every step?\n",
    "    **A:** Encourages **faster target reaching**.\n",
    "\n",
    "58. **Q:** What is the effect of reward shaping on learning?\n",
    "    **A:** Speeds up convergence by giving **progress feedback**.\n",
    "\n",
    "59. **Q:** What is the role of `n_steps` in PPO?\n",
    "    **A:** Number of steps to collect before policy update.\n",
    "\n",
    "60. **Q:** Explain `envs[0].env` in `play_one_episode()`.\n",
    "    **A:** Unwraps the vectorized environment to access **original Gym environment**.\n",
    "\n",
    "61. **Q:** Why use `np.random.default_rng(seed)` instead of `random`?\n",
    "    **A:** More consistent, **modern NumPy RNG**, supports reproducible results.\n",
    "\n",
    "62. **Q:** How does `_place_entities()` avoid overlapping robot and target?\n",
    "    **A:** Uses a loop that repeats sampling until positions differ.\n",
    "\n",
    "63. **Q:** Why use lists for `robot_pos`?\n",
    "    **A:** For **easy extension** to multiple robots in the future.\n",
    "\n",
    "64. **Q:** What is `shaping = 0.05*(prev_dist - new_dist)`?\n",
    "    **A:** Small positive reward for moving closer to target.\n",
    "\n",
    "65. **Q:** How is `step()` different from `reset()`?\n",
    "    **A:** `step()` moves robot and returns reward; `reset()` starts new episode.\n",
    "\n",
    "66. **Q:** What is printed in `play_one_episode()`?\n",
    "    **A:** ASCII grid and reward per step.\n",
    "\n",
    "67. **Q:** How does PPO handle continuous observation space?\n",
    "    **A:** `MlpPolicy` takes float observations directly.\n",
    "\n",
    "68. **Q:** Can this code work without PPO?\n",
    "    **A:** Yes, you can implement **Q-learning or random actions** for testing.\n",
    "\n",
    "69. **Q:** Why is `dtype=np.float32` important?\n",
    "    **A:** Ensures numerical stability for **tensor operations**.\n",
    "\n",
    "70. **Q:** How to make the grid larger?\n",
    "    **A:** Pass larger `width` and `height` when creating environment.\n",
    "\n",
    "---\n",
    "\n",
    "## **HARD QUESTIONS (71–100)**\n",
    "\n",
    "**71. Q:** Explain the purpose of `self._prev_dist` in `step()` function.\n",
    "**A:** It stores the Manhattan distance from the robot to the target in the previous step to calculate reward shaping, encouraging the robot to move closer.\n",
    "\n",
    "**72. Q:** Why do we use reward shaping with `shaping = 0.05 * (self._prev_dist - new_dist)`?\n",
    "**A:** Reward shaping provides intermediate feedback for the agent before reaching the goal, speeding up learning and reducing sparse reward issues.\n",
    "\n",
    "**73. Q:** Why is `truncated = self.steps >= self.max_steps` used?\n",
    "**A:** To stop the episode if the robot exceeds the maximum number of steps, preventing infinite loops.\n",
    "\n",
    "**74. Q:** What is the effect of `step_penalty = -0.1 if not self._done_robot else 0.0`?\n",
    "**A:** Penalizes the agent for every step taken before reaching the goal, encouraging faster completion.\n",
    "\n",
    "**75. Q:** How does `DummyVecEnv` help in PPO training?\n",
    "**A:** It allows vectorized environments, enabling multiple parallel environments or a single one for standardized API in Stable-Baselines3.\n",
    "\n",
    "**76. Q:** Why do we normalize positions with `_normalize(x, y)`?\n",
    "**A:** Normalization scales positions to \\[0,1], making it easier for the neural network policy to learn consistently regardless of grid size.\n",
    "\n",
    "**77. Q:** How does PPO handle exploration vs exploitation in this environment?\n",
    "**A:** PPO uses a stochastic policy during training, sampling actions from the learned probability distribution to balance exploration and exploitation.\n",
    "\n",
    "**78. Q:** Why is `action_space = spaces.Discrete(5)` used?\n",
    "**A:** Because the robot has 5 possible moves: stay, up, down, left, right.\n",
    "\n",
    "**79. Q:** Explain why `Monitor(env)` is used in `make_env_fn()`.\n",
    "**A:** It records episode statistics like rewards and lengths, which PPO uses for logging and evaluation.\n",
    "\n",
    "**80. Q:** What would happen if `self._done_robot` is not updated when the robot reaches the target?\n",
    "**A:** The episode would not terminate even if the goal is reached, and the agent could keep moving unnecessarily.\n",
    "\n",
    "**81. Q:** Why does `play_one_episode()` use `deterministic=True`?\n",
    "**A:** To let the trained model take the most probable action, showing the optimal learned policy without randomness.\n",
    "\n",
    "**82. Q:** How does the `_apply_move` method prevent the robot from leaving the grid?\n",
    "**A:** It clamps the new position within `[0, width-1]` and `[0, height-1]`.\n",
    "\n",
    "**83. Q:** What would happen if `obs_dim` is incorrectly set?\n",
    "**A:** The PPO policy network would fail because the input dimension wouldn't match the environment observation.\n",
    "\n",
    "**84. Q:** Why is `reward` cast to `float` in `step()`?\n",
    "**A:** PPO expects reward as a float, ensuring compatibility with Stable-Baselines3 tensor operations.\n",
    "\n",
    "**85. Q:** Can PPO train multiple robots with this code? Why or why not?\n",
    "**A:** No, the current environment asserts `num_robots == 1` for simplicity. Multi-robot support would need vectorized observation and action handling.\n",
    "\n",
    "**86. Q:** What is the purpose of the `_thunk` function in `make_env_fn()`?\n",
    "**A:** It creates a callable that returns a new environment instance for vectorized execution.\n",
    "\n",
    "**87. Q:** Why are the robot and target positions stored in a dictionary in `reset()`?\n",
    "**A:** For logging and debugging, providing clear information about initial positions.\n",
    "\n",
    "**88. Q:** How does PPO update its policy during training?\n",
    "**A:** PPO performs multiple epochs of gradient descent on clipped surrogate loss to improve policy stability.\n",
    "\n",
    "**89. Q:** What is the significance of `n_steps=128` in PPO?\n",
    "**A:** It defines how many environment steps are collected before updating the policy.\n",
    "\n",
    "**90. Q:** Why is `batch_size=32` used in PPO?\n",
    "**A:** It specifies the mini-batch size for stochastic gradient descent during policy updates.\n",
    "\n",
    "**91. Q:** What happens if `max_steps` is too small?\n",
    "**A:** Episodes may terminate before reaching the goal, making learning harder due to sparse positive rewards.\n",
    "\n",
    "**92. Q:** Explain the difference between `terminated` and `truncated` in `step()`.\n",
    "**A:** `terminated` signals goal completion, while `truncated` signals reaching max steps without goal completion.\n",
    "\n",
    "**93. Q:** How does PPO handle continuous vs discrete action spaces?\n",
    "**A:** In this environment, actions are discrete. PPO uses a Categorical policy for discrete actions, vs Gaussian for continuous.\n",
    "\n",
    "**94. Q:** Why do we pass `n_envs=1` in `make_vec_env` for this lightweight version?\n",
    "**A:** Single environment is sufficient for debugging; vectorization is optional and increases computational cost.\n",
    "\n",
    "**95. Q:** How does `_sample_free_cell()` ensure robot and target don’t overlap?\n",
    "**A:** It samples repeatedly until a free cell different from the robot’s position is found.\n",
    "\n",
    "**96. Q:** What would be the effect of removing reward shaping?\n",
    "**A:** Learning would slow down because the agent only receives reward at the goal, causing sparse feedback.\n",
    "\n",
    "**97. Q:** Why is `render()` returning a string instead of printing directly?\n",
    "**A:** Returning allows flexibility: print in console, log, or use in GUI without forcing direct output.\n",
    "\n",
    "**98. Q:** How can you extend this environment to multiple robots?\n",
    "**A:** Expand `robot_pos` to multiple entries, modify `observation_space` to include all robots, and handle collisions/rewards per robot.\n",
    "\n",
    "**99. Q:** Why is `np.random.default_rng(seed)` preferred over `np.random.seed()`?\n",
    "**A:** Provides a modern, faster, and more reliable random number generator for reproducibility.\n",
    "\n",
    "**100. Q:** Explain the role of `play_one_episode()` in the RL workflow.\n",
    "**A:** It tests a trained policy in the environment to visualize trajectories, debug behavior, and validate training performance.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1f6889-176d-4b80-8599-975c8e883ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fingerprint)",
   "language": "python",
   "name": "fingerprint-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
